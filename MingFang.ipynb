{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedAlziadi/1000MLProjects/blob/main/MingFang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnvQYIPssPXJ",
        "outputId": "dd987ae4-96db-4ea8-a3b9-20be0106765a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-059e5700b5a8>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mAttention\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_Adam_multiview\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMnistDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Attention'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from __future__ import print_function, division\n",
        "import argparse\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score as nmi_score\n",
        "from sklearn.metrics import adjusted_rand_score as ari_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import Linear\n",
        "from Attention import CAM\n",
        "from utils_Adam_multiview import MnistDataset, cluster_acc\n",
        "\n",
        "import os    #获取当前工作路径\n",
        "import pandas as pd   #将数据保存至相应文件中\n",
        "\n",
        "from pytorchtools import EarlyStopping\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def setup_seed(seed):  # 设置随机种子，用于实验结果复现\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "setup_seed(2023)"
      ],
      "id": "AnvQYIPssPXJ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fbtK9605kkNj"
      },
      "id": "fbtK9605kkNj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXIxQZ98sPXO"
      },
      "outputs": [],
      "source": [
        "class AE(nn.Module):\n",
        "\n",
        "    def __init__(self, n_enc_1, n_enc_2, n_enc_3, n_dec_1, n_dec_2, n_dec_3,\n",
        "                 n_input, n_z):\n",
        "        super(AE, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.enc_1 = Linear(n_input, n_enc_1)\n",
        "        self.enc_2 = Linear(n_enc_1, n_enc_2)\n",
        "        self.enc_3 = Linear(n_enc_2, n_enc_3)\n",
        "\n",
        "        self.z_layer = Linear(n_enc_3, n_z)\n",
        "\n",
        "        # decoder\n",
        "        self.dec_1 = Linear(n_z, n_dec_1)\n",
        "        self.dec_2 = Linear(n_dec_1, n_dec_2)\n",
        "        self.dec_3 = Linear(n_dec_2, n_dec_3)\n",
        "\n",
        "        self.x_bar_layer = Linear(n_dec_3, n_input)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # encoder\n",
        "        enc_h1 = F.relu(self.enc_1(x))\n",
        "        enc_h2 = F.relu(self.enc_2(enc_h1))\n",
        "        enc_h3 = F.relu(self.enc_3(enc_h2))\n",
        "\n",
        "        z = self.z_layer(enc_h3)\n",
        "\n",
        "        # decoder\n",
        "        dec_h1 = F.relu(self.dec_1(z))\n",
        "        dec_h2 = F.relu(self.dec_2(dec_h1))\n",
        "        dec_h3 = F.relu(self.dec_3(dec_h2))\n",
        "        x_bar = self.x_bar_layer(dec_h3)\n",
        "\n",
        "        return x_bar, z\n"
      ],
      "id": "vXIxQZ98sPXO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9fhb-E6sPXP",
        "outputId": "96f72a9f-046c-4a0c-df41-86598177edf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchvision\n",
            "  Downloading torchvision-0.14.1-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
            "Collecting typing-extensions\n",
            "  Downloading typing_extensions-4.6.3-py3-none-any.whl (31 kB)\n",
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp37-cp37m-win_amd64.whl (162.6 MB)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 425, in _error_catcher\n",
            "    yield\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 507, in read\n",
            "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
            "    data = self.__fp.read(amt)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\http\\client.py\", line 457, in read\n",
            "    n = self.readinto(b)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\http\\client.py\", line 501, in readinto\n",
            "    n = self.fp.readinto(b)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\socket.py\", line 589, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\ssl.py\", line 1071, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\ssl.py\", line 929, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "socket.timeout: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 186, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 331, in run\n",
            "    resolver.resolve(requirement_set)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 177, in resolve\n",
            "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 333, in _resolve_one\n",
            "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_internal\\legacy_resolve.py\", line 282, in _get_abstract_dist_for\n",
            "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 482, in prepare_linked_requirement\n",
            "    hashes=hashes,\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 287, in unpack_url\n",
            "    hashes=hashes,\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 159, in unpack_http_url\n",
            "    link, downloader, temp_dir.path, hashes\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 303, in _download_http_url\n",
            "    for chunk in download.chunks:\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_internal\\utils\\ui.py\", line 160, in iter\n",
            "    for x in it:\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 39, in response_chunks\n",
            "    decode_content=False,\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 564, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 529, in read\n",
            "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\contextlib.py\", line 130, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"C:\\Users\\Ahmed\\anaconda3\\anaconda.3.1\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 430, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision\n"
      ],
      "id": "-9fhb-E6sPXP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWwC6VBSsPXP"
      },
      "outputs": [],
      "source": [
        "class MAE(nn.Module):\n",
        "\n",
        "    def __init__(self, n_enc_1 , n_enc_2, n_enc_3, n_dec_1, n_dec_2, n_dec_3, n_input, in_channels, n_z):\n",
        "        super(MAE, self).__init__()\n",
        "        # self.n_enc_1 = n_enc_1,\n",
        "        # self.n_enc_2 = n_enc_2,\n",
        "        # self.n_enc_3 = n_enc_3,\n",
        "        # self.n_dec_1 = n_dec_1,\n",
        "        # self.n_dec_2 = n_dec_2,\n",
        "        # self.n_dec_3 = n_dec_3,\n",
        "        # self.n_input = n_input,\n",
        "        # self.n_z = n_z,\n",
        "        self.ae1 = AE(\n",
        "            n_enc_1=n_enc_1,\n",
        "            n_enc_2=n_enc_2,\n",
        "            n_enc_3=n_enc_3,\n",
        "            n_dec_1=n_dec_1,\n",
        "            n_dec_2=n_dec_2,\n",
        "            n_dec_3=n_dec_3,\n",
        "            n_input=n_input,\n",
        "            n_z=n_z)\n",
        "        self.ae2 = AE(\n",
        "             n_enc_1=n_enc_1,\n",
        "            n_enc_2=n_enc_2,\n",
        "            n_enc_3=n_enc_3,\n",
        "            n_dec_1=n_dec_1,\n",
        "            n_dec_2=n_dec_2,\n",
        "            n_dec_3=n_dec_3,\n",
        "            n_input=n_input,\n",
        "            n_z=n_z)\n",
        "        self.ae3 = AE(\n",
        "            n_enc_1=n_enc_1,\n",
        "            n_enc_2=n_enc_2,\n",
        "            n_enc_3=n_enc_3,\n",
        "            n_dec_1=n_dec_1,\n",
        "            n_dec_2=n_dec_2,\n",
        "            n_dec_3=n_dec_3,\n",
        "            n_input=n_input,\n",
        "            n_z=n_z)\n",
        "        self.ae4 = AE(\n",
        "            n_enc_1=n_enc_1,\n",
        "            n_enc_2=n_enc_2,\n",
        "            n_enc_3=n_enc_3,\n",
        "            n_dec_1=n_dec_1,\n",
        "            n_dec_2=n_dec_2,\n",
        "            n_dec_3=n_dec_3,\n",
        "            n_input=n_input,\n",
        "            n_z=n_z)\n",
        "        self.ae5 = AE(\n",
        "            n_enc_1=n_enc_1,\n",
        "            n_enc_2=n_enc_2,\n",
        "            n_enc_3=n_enc_3,\n",
        "            n_dec_1=n_dec_1,\n",
        "            n_dec_2=n_dec_2,\n",
        "            n_dec_3=n_dec_3,\n",
        "            n_input=n_input,\n",
        "            n_z=n_z)\n",
        "        self.ae6 = AE(\n",
        "            n_enc_1=n_enc_1,\n",
        "            n_enc_2=n_enc_2,\n",
        "            n_enc_3=n_enc_3,\n",
        "            n_dec_1=n_dec_1,\n",
        "            n_dec_2=n_dec_2,\n",
        "            n_dec_3=n_dec_3,\n",
        "            n_input=n_input,\n",
        "            n_z=n_z)\n",
        "        self.ae7 = AE(\n",
        "            n_enc_1=n_enc_1,\n",
        "            n_enc_2=n_enc_2,\n",
        "            n_enc_3=n_enc_3,\n",
        "            n_dec_1=n_dec_1,\n",
        "            n_dec_2=n_dec_2,\n",
        "            n_dec_3=n_dec_3,\n",
        "            n_input=n_input,\n",
        "            n_z=n_z)\n",
        "        self.ae8 = AE(\n",
        "            n_enc_1=n_enc_1,\n",
        "            n_enc_2=n_enc_2,\n",
        "            n_enc_3=n_enc_3,\n",
        "            n_dec_1=n_dec_1,\n",
        "            n_dec_2=n_dec_2,\n",
        "            n_dec_3=n_dec_3,\n",
        "            n_input=n_input,\n",
        "            n_z=n_z)\n",
        "\n",
        "        self.cam = CAM(in_channels=in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_bar1, z1 = self.ae1(x[:,0,:])\n",
        "        x_bar2, z2 = self.ae2(x[:,1,:])\n",
        "        x_bar3, z3 = self.ae3(x[:,2,:])\n",
        "        x_bar4, z4 = self.ae4(x[:,3,:])\n",
        "        x_bar5, z5 = self.ae5(x[:,4,:])\n",
        "        x_bar6, z6 = self.ae6(x[:,5,:])\n",
        "        x_bar7, z7 = self.ae7(x[:,6,:])\n",
        "        x_bar8, z8 = self.ae8(x[:,7,:])\n",
        "#         print(f'z1.shape: {z1.shape}')\n",
        "        loss = F.mse_loss(x_bar1, x[:, 0, :]) + F.mse_loss(x_bar2, x[:, 1, :]) + F.mse_loss(x_bar3, x[:, 2, :])\n",
        "        + F.mse_loss(x_bar4, x[:, 3, :]) + F.mse_loss(x_bar5, x[:, 4, :]) + F.mse_loss(x_bar6, x[:, 5, :])\n",
        "        + F.mse_loss(x_bar7, x[:, 6, :]) + F.mse_loss(x_bar8, x[:, 7, :])\n",
        "        # 多视图融合\n",
        "        z_cat = torch.cat((z1.unsqueeze(1), z2.unsqueeze(1), z3.unsqueeze(1), z4.unsqueeze(1),\n",
        "                           z5.unsqueeze(1), z6.unsqueeze(1), z7.unsqueeze(1), z8.unsqueeze(1)), dim=1)\n",
        "#         print(f'z_cat.shape: {z_cat.shape}')\n",
        "#         z = torch.sum(z_cat,dim=1)\n",
        "        Cam = self.cam(z_cat)\n",
        "        z = Cam[0].squeeze(1)\n",
        "#         print(f'z.shape: {z.shape}')\n",
        "        c_weights = Cam[1]\n",
        "#         print(f'c_weights.shape: {c_weights.shape}')\n",
        "#         print(net.state_dict()['module.conv1.1.bias'])  # 填该层参数名\n",
        "\n",
        "        embedding = [z1, z2, z3, z4, z5, z6, z7, z8]\n",
        "        # Loss_constraint1,embedding之间的MSE，不过这里符号要取负,考虑视图特征之间的多样性\n",
        "        L_con1 = 0\n",
        "        for i in range(len(embedding)):\n",
        "            for j in range(i+1, len(embedding)):\n",
        "                L_con1 += F.mse_loss(embedding[i], embedding[j])\n",
        "        # Loss_constraint2,embedding与一致性embedding之间的MSE,考虑视图特征之间的一致性\n",
        "        L_con2 = 0\n",
        "        for i in range(len(embedding)):\n",
        "            L_con2 += F.mse_loss(z, embedding[i])\n",
        "\n",
        "        return z, loss, L_con1, L_con2, c_weights\n"
      ],
      "id": "YWwC6VBSsPXP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPoZRwJpsPXQ"
      },
      "outputs": [],
      "source": [
        "class IDEC(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_enc_1,\n",
        "                 n_enc_2,\n",
        "                 n_enc_3,\n",
        "                 n_dec_1,\n",
        "                 n_dec_2,\n",
        "                 n_dec_3,\n",
        "                 n_input,\n",
        "                 n_z,\n",
        "                 in_channels,\n",
        "                 n_clusters,\n",
        "                 alpha=1,\n",
        "                 pretrain_path='ae_Adam_multiview.pkl'):\n",
        "        super(IDEC, self).__init__()\n",
        "        self.alpha = 1.0\n",
        "        self.pretrain_path = pretrain_path\n",
        "\n",
        "#         self.ae = AE(\n",
        "#             n_enc_1=n_enc_1,\n",
        "#             n_enc_2=n_enc_2,\n",
        "#             n_enc_3=n_enc_3,\n",
        "#             n_dec_1=n_dec_1,\n",
        "#             n_dec_2=n_dec_2,\n",
        "#             n_dec_3=n_dec_3,\n",
        "#             n_input=n_input,\n",
        "#             n_z=n_z)\n",
        "\n",
        "        self.ae = MAE(\n",
        "            n_enc_1=n_enc_1,\n",
        "            n_enc_2=n_enc_2,\n",
        "            n_enc_3=n_enc_3,\n",
        "            n_dec_1=n_dec_1,\n",
        "            n_dec_2=n_dec_2,\n",
        "            n_dec_3=n_dec_3,\n",
        "            n_input=n_input,\n",
        "            in_channels=in_channels,\n",
        "            n_z=n_z)\n",
        "\n",
        "#         self.cam = CAM(in_channels=in_channels)\n",
        "\n",
        "        # cluster layer\n",
        "        self.cluster_layer = Parameter(torch.Tensor(n_clusters, n_z))\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.cluster_layer.data)\n",
        "\n",
        "    def pretrain(self, path=''):\n",
        "        if path == '':\n",
        "            pretrain_ae(self.ae)\n",
        "\n",
        "        # load pretrain weights\n",
        "        self.ae.load_state_dict(torch.load(self.pretrain_path))\n",
        "        print('load pretrained ae from', path)\n",
        "\n",
        "\n",
        "#     def forward(self, x):\n",
        "\n",
        "#         x_bar, z = self.ae(x)\n",
        "#         # cluster\n",
        "#         q = 1.0 / (1.0 + torch.sum(\n",
        "#             torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.alpha)\n",
        "#         q = q.pow((self.alpha + 1.0) / 2.0)\n",
        "#         q = (q.t() / torch.sum(q, 1)).t()\n",
        "#         return x_bar, q\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x_bar1, z1 = self.ae(x[:, 0, :])\n",
        "#         x_bar2, z2 = self.ae(x[:, 1, :])\n",
        "#         x_bar3, z3 = self.ae(x[:, 2, :])\n",
        "#         x_bar4, z4 = self.ae(x[:, 3, :])\n",
        "#         x_bar5, z5 = self.ae(x[:, 4, :])\n",
        "#         x_bar6, z6 = self.ae(x[:, 5, :])\n",
        "#         x_bar7, z7 = self.ae(x[:, 6, :])\n",
        "#         x_bar8, z8 = self.ae(x[:, 7, :])\n",
        "#         x_bar = torch.concat((x_bar1,x_bar2,x_bar3,x_bar4,x_bar5,x_bar6,x_bar7,x_bar8),dim=1)\n",
        "\n",
        "#         z = torch.concat((z1,z2,z3,z4,z5,z6,z7,z8),dim=1)\n",
        "\n",
        "#         # cluster\n",
        "#         q = 1.0 / (1.0 + torch.sum(\n",
        "#             torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.alpha)\n",
        "#         q = q.pow((self.alpha + 1.0) / 2.0)\n",
        "#         q = (q.t() / torch.sum(q, 1)).t()\n",
        "\n",
        "#         return x_bar, q\n",
        "\n",
        "    def forward(self, x):\n",
        "        ae = self.ae(x)\n",
        "        loss = ae[1]\n",
        "        L_con1 = ae[2]\n",
        "        L_con2 = ae[3]\n",
        "        z = ae[0]\n",
        "        c_weights = ae[4]\n",
        "#         z_cat = torch.cat((ae[0].unsqueeze(1), ae[1].unsqueeze(1), ae[2].unsqueeze(1), ae[3].unsqueeze(1),\n",
        "#                            ae[4].unsqueeze(1), ae[5].unsqueeze(1), ae[6].unsqueeze(1), ae[7].unsqueeze(1)), dim=1)\n",
        "        # print(f\"ae[0].shape:{ae[0].shape}\")\n",
        "        # print(f\"z_cat.shape:{z_cat.shape}\")\n",
        "#         z = self.cam(z_cat).squeeze(1)\n",
        "        # print(f\"z.shape:{z.shape}\")\n",
        "        # cluster\n",
        "        q = 1.0 / (1.0 + torch.sum(\n",
        "            torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.alpha)\n",
        "        q = q.pow((self.alpha + 1.0) / 2.0)\n",
        "        q = (q.t() / torch.sum(q, 1)).t()\n",
        "        return z, q, loss, L_con1, L_con2, c_weights\n"
      ],
      "id": "NPoZRwJpsPXQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lif1bkolsPXR"
      },
      "outputs": [],
      "source": [
        "def target_distribution(q):\n",
        "    weight = q**2 / q.sum(0)\n",
        "    return (weight.t() / weight.sum(1)).t()"
      ],
      "id": "lif1bkolsPXR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK5Ih0fNsPXS"
      },
      "outputs": [],
      "source": [
        "def pretrain_ae(model):\n",
        "    '''\n",
        "    pretrain autoencoder\n",
        "    '''\n",
        "    train_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)\n",
        "    print(model)\n",
        "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
        "    early_stopping = EarlyStopping(patience=10, verbose=True)\n",
        "#     ACC = []\n",
        "#     ARI = []\n",
        "#     NMI = []\n",
        "#     lambda1 = args.lambda1\n",
        "#     lambda2 = args.lambda2\n",
        "    for epoch in range(100):\n",
        "        total_loss = 0.\n",
        "#         z_all = []\n",
        "        B_Y = []\n",
        "        for batch_idx, (x, b_y, _) in enumerate(train_loader): # x: torch.Size([256, 8, 2048]), b_y: torch.Size([256])\n",
        "            x = x.to(device)\n",
        "            b_y = b_y.to(device)\n",
        "            # optimizer.zero_grad()\n",
        "            # x_bar, z = model(x)\n",
        "            # loss = F.mse_loss(x_bar, x)\n",
        "            # total_loss += loss.item()\n",
        "            #\n",
        "            # loss.backward()\n",
        "            # optimizer.step()\n",
        "\n",
        "            B_Y.append(b_y)\n",
        "            optimizer.zero_grad()\n",
        "            z, loss, L_con1, L_con2, c_weights = model(x)\n",
        "            total_loss += loss.item()\n",
        "#             z_all.append(z)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "#     print(z_all[-1])\n",
        "\n",
        "\n",
        "#         kmeans = KMeans(n_clusters=args.n_clusters, n_init=30)\n",
        "#         y_pred = kmeans.fit_predict(torch.cat(z_all).data.cpu().numpy())\n",
        "#         # print(f\"torch.cat(z_all).shape:{torch.cat(z_all).shape}\")\n",
        "#         # print(f\"torch.cat(B_Y).shape:{torch.cat(B_Y).shape}\")\n",
        "#         # print(f\"Y.shape:{Y.shape}\")\n",
        "#         # print(f\"y_pred.shape:{y_pred.shape}\")\n",
        "#         acc = cluster_acc(torch.cat(B_Y).cpu().numpy(), y_pred)\n",
        "#         nmi = nmi_score(torch.cat(B_Y).cpu().numpy(), y_pred)\n",
        "#         ari = ari_score(torch.cat(B_Y).cpu().numpy(), y_pred)\n",
        "#         print('Iter {}'.format(epoch), ':Acc {:.4f}'.format(acc),\n",
        "#               ', nmi {:.4f}'.format(nmi), ', ari {:.4f}'.format(ari))\n",
        "\n",
        "#         ACC.append(acc)\n",
        "#         NMI.append(nmi)\n",
        "#         ARI.append(ari)\n",
        "\n",
        "        print(\"epoch {} loss={:.4f}\".format(epoch,\n",
        "                                            total_loss / (batch_idx + 1)))\n",
        "\n",
        "        early_stopping(total_loss / (batch_idx + 1), model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "        torch.save(model.state_dict(), args.pretrain_path)\n",
        "    print(\"model saved to {}.\".format(args.pretrain_path))\n",
        "#     return z_all[-1]\n",
        "\n",
        "#     print(f\"ACC_MAX:{max(ACC)}\")\n",
        "#     print(f\"NMI_MAX:{max(NMI)}\")\n",
        "#     print(f\"ARI_MAX:{max(ARI)}\")\n",
        "\n",
        "#     file = os.getcwd() + 'ae_Adam_multiview_F3.csv'    #保存文件位置，即当前工作路径下的csv文件\n",
        "#     data = pd.DataFrame({'ACC_MAX': [max(ACC)], 'NMI_MAX': [max(NMI)],'ARI_MAX': [max(ARI)],\n",
        "#                          'LR': [args.lr],\n",
        "'Hidden_z': [args.n_z], 'Layer1': [args.layer1], 'Layer2': [args.layer2], 'gamma': [args.gamma], 'lambda1': [args.lambda1], 'lambda2': [args.lambda2]})  # 要保存的数据\n",
        "#     data.to_csv(file, index=True, mode='a+')  # 数据写入，index=False表示不加索引"
      ],
      "id": "bK5Ih0fNsPXS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlt5jb4AsPXS"
      },
      "outputs": [],
      "source": [
        "def train_idec():\n",
        "\n",
        "    model = IDEC(\n",
        "        n_enc_1=args.layer1,\n",
        "        n_enc_2=args.layer1,\n",
        "        n_enc_3=args.layer2,\n",
        "        n_dec_1=args.layer2,\n",
        "        n_dec_2=args.layer1,\n",
        "        n_dec_3=args.layer1,\n",
        "        n_input=args.n_input,\n",
        "        n_z=args.n_z,\n",
        "        in_channels=args.in_channels,\n",
        "        n_clusters=args.n_clusters,\n",
        "        alpha=1.0,\n",
        "        pretrain_path=args.pretrain_path).to(device)\n",
        "\n",
        "    #  model.pretrain('ae_Adam_multiview.pkl')\n",
        "    model.pretrain()\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        dataset, batch_size=args.batch_size, shuffle=False)\n",
        "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
        "\n",
        "#     z_all = []\n",
        "\n",
        "    # cluster parameter initiate\n",
        "    data = dataset.x\n",
        "    y = dataset.y\n",
        "    data = torch.Tensor(data).to(device)\n",
        "    hidden, _, _, _, _, c_weights = model(data)\n",
        "#     z_all.append(hidden)\n",
        "    kmeans = KMeans(n_clusters=args.n_clusters, n_init=20)\n",
        "    y_pred = kmeans.fit_predict(hidden.data.cpu().numpy())\n",
        "#     nmi_k = nmi_score(y_pred, y)\n",
        "#     print(\"nmi score={:.4f}\".format(nmi_k))\n",
        "\n",
        "    acc_k = cluster_acc(y, y_pred)\n",
        "    nmi_k = nmi_score(y, y_pred)\n",
        "    ari_k = ari_score(y, y_pred)\n",
        "\n",
        "    print( ':acc={:.4f}'.format(acc_k),', nmi={:.4f}'.format(nmi_k), ', ari={:.4f}'.format(ari_k))\n",
        "#     print(f\"z_init:{z_all[-1]}\",f\"z_init.shape:{z_all[-1].shape}\")\n",
        "#     z_init = z_all[-1].cpu().data.numpy()\n",
        "#     np.savetxt('_seed2023_init_multiview_可视化F44.csv', z_init, delimiter=',')\n",
        "#     file = os.getcwd() + '_seed2023_init_multiview_可视化F44.csv'    #保存文件位置，即当前工作路径下的csv文件\n",
        "#     DATA = pd.DataFrame({'ACC_init': [acc_k], 'NMI_init': [nmi_k],'ARI_init': [ari_k],\n",
        "# #                          'z_init': [z_all[-1]],\n",
        "#                          'LR': [args.lr], 'Hidden_z': [args.n_z], 'Layer1': [args.layer1], 'Layer2': [args.layer2], 'gamma': [args.gamma], 'lambda1': [args.lambda1], 'lambda2': [args.lambda2]})  # 要保存的数据\n",
        "#     DATA.to_csv(file, index=True, mode='a+')  # 数据写入，index=False表示不加索引\n",
        "\n",
        "    hidden = None\n",
        "#     x_bar = None\n",
        "\n",
        "    y_pred_last = y_pred\n",
        "    model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
        "\n",
        "    model.train()\n",
        "    ACC = []\n",
        "    ARI = []\n",
        "    NMI = []\n",
        "#     Z_ALL = []\n",
        "    C_WEIGHTS = []\n",
        "    lambda1 = args.lambda1\n",
        "    lambda2 = args.lambda2\n",
        "    for epoch in range(10): # 200\n",
        "\n",
        "        if epoch % args.update_interval == 0:\n",
        "\n",
        "            Z, tmp_q, _, _, _, c_weights = model(data)\n",
        "\n",
        "            # update target distribution p\n",
        "            tmp_q = tmp_q.data\n",
        "            p = target_distribution(tmp_q)\n",
        "\n",
        "            # evaluate clustering performance\n",
        "            y_pred = tmp_q.cpu().numpy().argmax(1)\n",
        "            delta_label = np.sum(y_pred != y_pred_last).astype(\n",
        "                np.float32) / y_pred.shape[0]\n",
        "            y_pred_last = y_pred\n",
        "\n",
        "            acc = cluster_acc(y, y_pred)\n",
        "            nmi = nmi_score(y, y_pred)\n",
        "            ari = ari_score(y, y_pred)\n",
        "            print('Iter {}'.format(epoch), ':Acc {:.4f}'.format(acc),\n",
        "                  ', nmi {:.4f}'.format(nmi), ', ari {:.4f}'.format(ari))\n",
        "            ACC.append(acc)\n",
        "            NMI.append(nmi)\n",
        "            ARI.append(ari)\n",
        "#             Z_ALL.append(Z)\n",
        "            C_WEIGHTS.append(c_weights)\n",
        "            # if epoch > 0 and delta_label < args.tol:\n",
        "            #     print('delta_label {:.4f}'.format(delta_label), '< tol',\n",
        "            #           args.tol)\n",
        "            #     print('Reached tolerance threshold. Stopping training.')\n",
        "            #     break\n",
        "        for batch_idx, (x, _, idx) in enumerate(train_loader):\n",
        "\n",
        "            x = x.to(device)\n",
        "            idx = idx.to(device)\n",
        "\n",
        "#             x_bar, q = model(x)\n",
        "\n",
        "#             reconstr_loss = F.mse_loss(x_bar, x)\n",
        "\n",
        "            _, q, re_loss, L_con1, L_con2, c_weights = model(x)\n",
        "\n",
        "#             C_WEIGHTS.append(c_weights)\n",
        "#             print(c_weights.shape)\n",
        "#             print(np.array(C_WEIGHTS).shape)\n",
        "\n",
        "            ae_loss = re_loss-lambda1*L_con1+lambda2*L_con2\n",
        "\n",
        "            kl_loss = F.kl_div(q.log(), p[idx.long()])\n",
        "            loss = args.gamma * kl_loss + ae_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"ACC_MAX:{max(ACC)}\")\n",
        "    index_max = ACC.index(max(ACC))\n",
        "    print(f\"NMI_MAX:{NMI[index_max]}\")\n",
        "    print(f\"ARI_MAX:{ARI[index_max]}\")\n",
        "#     print(f\"z_finetuning:{Z_ALL[index_max]}\",f\"z_finetuning.shape:{Z_ALL[index_max].shape}\")\n",
        "#     z_finetuning = Z_ALL[index_max].cpu().data.numpy()\n",
        "#     np.savetxt('_seed2023_finetuning_multiview_可视化F44.csv', z_finetuning, delimiter=',')\n",
        "\n",
        "#     print(f\"C_weights:{C_WEIGHTS[(index_max):((index_max+1))]}\")\n",
        "#     print(f\"C_weights:{C_WEIGHTS[(5110*index_max):(5110*(index_max+1))]}\")\n",
        "#     C_weights = np.array(C_WEIGHTS[(5110*(index_max+1)-10):(5110*(index_max+1))])#.reshape(5110,8)\n",
        "    C_weights = np.array(C_WEIGHTS[((index_max)):(index_max+1)])#.reshape(5110,8)\n",
        "    print(C_weights[0].shape)\n",
        "    file = os.getcwd() + 'fffffggggg_seed2023_IDEC_multiview_通道权重F1.csv'    #保存文件位置，即当前工作路径下的csv文件\n",
        "    for i in range(5110):\n",
        "        data = pd.DataFrame({'ACC_MAX': [max(ACC)], 'NMI_MAX': [NMI[index_max]],'ARI_MAX': [ARI[index_max]],\n",
        "#                          'C_weights': [C_weights],\n",
        "                             'C1_weights': [C_weights[0][i,0,:]],'C2_weights': [C_weights[0][i,1,:]],'C3_weights': [C_weights[0][i,2,:]],'C4_weights': [C_weights[0][i,3,:]],\n",
        "                             'C5_weights': [C_weights[0][i,4,:]],'C6_weights': [C_weights[0][i,5,:]],'C7_weights': [C_weights[0][i,6,:]],'C8_weights': [C_weights[0][i,7,:]],\n",
        "                         'LR': [args.lr], 'Hidden_z': [args.n_z], 'Layer1': [args.layer1], 'Layer2': [args.layer2], 'gamma': [args.gamma], 'lambda1': [args.lambda1], 'lambda2': [args.lambda2]})\n",
        "#     data = pd.DataFrame({'ACC_MAX': [max(ACC)], 'NMI_MAX': [NMI[index_max]],'ARI_MAX': [ARI[index_max]],'c_weights': [C_WEIGHTS[(5110*index_max):(5110*(index_max+1))]],\n",
        "#                          'LR': [args.lr], 'Hidden_z': [args.n_z], 'Layer1': [args.layer1], 'Layer2': [args.layer2], 'gamma': [args.gamma], 'lambda1': [args.lambda1], 'lambda2': [args.lambda2]})  # 要保存的数据\n",
        "        data.to_csv(file, index=True, mode='a+')  # 数据写入，index=False表示不加索引\n",
        "        # 打印所有参数\n",
        "#     for name, parameters in model.named_parameters():\n",
        "#         print(name, ':', parameters)"
      ],
      "id": "dlt5jb4AsPXS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwDnqvBwsPXT",
        "outputId": "c8017aa3-cd7a-4325-a733-6defa16ad6b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "use cuda: True\n",
            "data samples (5110, 8, 2048)\n",
            "Namespace(batch_size=256, cuda=True, dataset='mnist', gamma=100, in_channels=8, lambda1=0.001, lambda2=0.001, layer1=500, layer2=1600, lr=0.0001, n_clusters=5, n_input=2048, n_z=20, pretrain_path='ae_Adam_multiview.pkl', tol=0.001, update_interval=1)\n",
            "MAE(\n",
            "  (ae1): AE(\n",
            "    (enc_1): Linear(in_features=2048, out_features=500, bias=True)\n",
            "    (enc_2): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (enc_3): Linear(in_features=500, out_features=1600, bias=True)\n",
            "    (z_layer): Linear(in_features=1600, out_features=20, bias=True)\n",
            "    (dec_1): Linear(in_features=20, out_features=1600, bias=True)\n",
            "    (dec_2): Linear(in_features=1600, out_features=500, bias=True)\n",
            "    (dec_3): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (x_bar_layer): Linear(in_features=500, out_features=2048, bias=True)\n",
            "  )\n",
            "  (ae2): AE(\n",
            "    (enc_1): Linear(in_features=2048, out_features=500, bias=True)\n",
            "    (enc_2): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (enc_3): Linear(in_features=500, out_features=1600, bias=True)\n",
            "    (z_layer): Linear(in_features=1600, out_features=20, bias=True)\n",
            "    (dec_1): Linear(in_features=20, out_features=1600, bias=True)\n",
            "    (dec_2): Linear(in_features=1600, out_features=500, bias=True)\n",
            "    (dec_3): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (x_bar_layer): Linear(in_features=500, out_features=2048, bias=True)\n",
            "  )\n",
            "  (ae3): AE(\n",
            "    (enc_1): Linear(in_features=2048, out_features=500, bias=True)\n",
            "    (enc_2): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (enc_3): Linear(in_features=500, out_features=1600, bias=True)\n",
            "    (z_layer): Linear(in_features=1600, out_features=20, bias=True)\n",
            "    (dec_1): Linear(in_features=20, out_features=1600, bias=True)\n",
            "    (dec_2): Linear(in_features=1600, out_features=500, bias=True)\n",
            "    (dec_3): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (x_bar_layer): Linear(in_features=500, out_features=2048, bias=True)\n",
            "  )\n",
            "  (ae4): AE(\n",
            "    (enc_1): Linear(in_features=2048, out_features=500, bias=True)\n",
            "    (enc_2): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (enc_3): Linear(in_features=500, out_features=1600, bias=True)\n",
            "    (z_layer): Linear(in_features=1600, out_features=20, bias=True)\n",
            "    (dec_1): Linear(in_features=20, out_features=1600, bias=True)\n",
            "    (dec_2): Linear(in_features=1600, out_features=500, bias=True)\n",
            "    (dec_3): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (x_bar_layer): Linear(in_features=500, out_features=2048, bias=True)\n",
            "  )\n",
            "  (ae5): AE(\n",
            "    (enc_1): Linear(in_features=2048, out_features=500, bias=True)\n",
            "    (enc_2): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (enc_3): Linear(in_features=500, out_features=1600, bias=True)\n",
            "    (z_layer): Linear(in_features=1600, out_features=20, bias=True)\n",
            "    (dec_1): Linear(in_features=20, out_features=1600, bias=True)\n",
            "    (dec_2): Linear(in_features=1600, out_features=500, bias=True)\n",
            "    (dec_3): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (x_bar_layer): Linear(in_features=500, out_features=2048, bias=True)\n",
            "  )\n",
            "  (ae6): AE(\n",
            "    (enc_1): Linear(in_features=2048, out_features=500, bias=True)\n",
            "    (enc_2): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (enc_3): Linear(in_features=500, out_features=1600, bias=True)\n",
            "    (z_layer): Linear(in_features=1600, out_features=20, bias=True)\n",
            "    (dec_1): Linear(in_features=20, out_features=1600, bias=True)\n",
            "    (dec_2): Linear(in_features=1600, out_features=500, bias=True)\n",
            "    (dec_3): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (x_bar_layer): Linear(in_features=500, out_features=2048, bias=True)\n",
            "  )\n",
            "  (ae7): AE(\n",
            "    (enc_1): Linear(in_features=2048, out_features=500, bias=True)\n",
            "    (enc_2): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (enc_3): Linear(in_features=500, out_features=1600, bias=True)\n",
            "    (z_layer): Linear(in_features=1600, out_features=20, bias=True)\n",
            "    (dec_1): Linear(in_features=20, out_features=1600, bias=True)\n",
            "    (dec_2): Linear(in_features=1600, out_features=500, bias=True)\n",
            "    (dec_3): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (x_bar_layer): Linear(in_features=500, out_features=2048, bias=True)\n",
            "  )\n",
            "  (ae8): AE(\n",
            "    (enc_1): Linear(in_features=2048, out_features=500, bias=True)\n",
            "    (enc_2): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (enc_3): Linear(in_features=500, out_features=1600, bias=True)\n",
            "    (z_layer): Linear(in_features=1600, out_features=20, bias=True)\n",
            "    (dec_1): Linear(in_features=20, out_features=1600, bias=True)\n",
            "    (dec_2): Linear(in_features=1600, out_features=500, bias=True)\n",
            "    (dec_3): Linear(in_features=500, out_features=500, bias=True)\n",
            "    (x_bar_layer): Linear(in_features=500, out_features=2048, bias=True)\n",
            "  )\n",
            "  (cam): CAM(\n",
            "    (GAP): Sequential(\n",
            "      (0): AdaptiveAvgPool1d(output_size=1)\n",
            "    )\n",
            "    (CNN1): Sequential(\n",
            "      (0): Conv1d(8, 4, kernel_size=(1,), stride=(1,))\n",
            "      (1): Tanh()\n",
            "    )\n",
            "    (CNN2): Sequential(\n",
            "      (0): Conv1d(4, 8, kernel_size=(1,), stride=(1,))\n",
            "      (1): Tanh()\n",
            "    )\n",
            "    (GMP): Sequential(\n",
            "      (0): AdaptiveMaxPool1d(output_size=1)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "epoch 0 loss=1.5017\n",
            "Validation loss decreased (inf --> 1.501697).  Saving model ...\n",
            "epoch 1 loss=1.4968\n",
            "Validation loss decreased (1.501697 --> 1.496756).  Saving model ...\n",
            "epoch 2 loss=1.4733\n",
            "Validation loss decreased (1.496756 --> 1.473314).  Saving model ...\n",
            "epoch 3 loss=1.4438\n",
            "Validation loss decreased (1.473314 --> 1.443781).  Saving model ...\n",
            "epoch 4 loss=1.4256\n",
            "Validation loss decreased (1.443781 --> 1.425598).  Saving model ...\n",
            "epoch 5 loss=1.4131\n",
            "Validation loss decreased (1.425598 --> 1.413150).  Saving model ...\n",
            "epoch 6 loss=1.4050\n",
            "Validation loss decreased (1.413150 --> 1.405019).  Saving model ...\n",
            "epoch 7 loss=1.4002\n",
            "Validation loss decreased (1.405019 --> 1.400243).  Saving model ...\n",
            "epoch 8 loss=1.3967\n",
            "Validation loss decreased (1.400243 --> 1.396673).  Saving model ...\n",
            "epoch 9 loss=1.3929\n",
            "Validation loss decreased (1.396673 --> 1.392941).  Saving model ...\n",
            "epoch 10 loss=1.3887\n",
            "Validation loss decreased (1.392941 --> 1.388696).  Saving model ...\n",
            "epoch 11 loss=1.3839\n",
            "Validation loss decreased (1.388696 --> 1.383918).  Saving model ...\n",
            "epoch 12 loss=1.3787\n",
            "Validation loss decreased (1.383918 --> 1.378680).  Saving model ...\n",
            "epoch 13 loss=1.3735\n",
            "Validation loss decreased (1.378680 --> 1.373540).  Saving model ...\n",
            "epoch 14 loss=1.3685\n",
            "Validation loss decreased (1.373540 --> 1.368458).  Saving model ...\n",
            "epoch 15 loss=1.3636\n",
            "Validation loss decreased (1.368458 --> 1.363573).  Saving model ...\n",
            "epoch 16 loss=1.3590\n",
            "Validation loss decreased (1.363573 --> 1.359017).  Saving model ...\n",
            "epoch 17 loss=1.3546\n",
            "Validation loss decreased (1.359017 --> 1.354581).  Saving model ...\n",
            "epoch 18 loss=1.3503\n",
            "Validation loss decreased (1.354581 --> 1.350251).  Saving model ...\n",
            "epoch 19 loss=1.3461\n",
            "Validation loss decreased (1.350251 --> 1.346091).  Saving model ...\n",
            "epoch 20 loss=1.3421\n",
            "Validation loss decreased (1.346091 --> 1.342069).  Saving model ...\n",
            "epoch 21 loss=1.3382\n",
            "Validation loss decreased (1.342069 --> 1.338195).  Saving model ...\n",
            "epoch 22 loss=1.3344\n",
            "Validation loss decreased (1.338195 --> 1.334441).  Saving model ...\n",
            "epoch 23 loss=1.3309\n",
            "Validation loss decreased (1.334441 --> 1.330918).  Saving model ...\n",
            "epoch 24 loss=1.3275\n",
            "Validation loss decreased (1.330918 --> 1.327461).  Saving model ...\n",
            "epoch 25 loss=1.3241\n",
            "Validation loss decreased (1.327461 --> 1.324138).  Saving model ...\n",
            "epoch 26 loss=1.3210\n",
            "Validation loss decreased (1.324138 --> 1.321035).  Saving model ...\n",
            "epoch 27 loss=1.3180\n",
            "Validation loss decreased (1.321035 --> 1.317989).  Saving model ...\n",
            "epoch 28 loss=1.3152\n",
            "Validation loss decreased (1.317989 --> 1.315174).  Saving model ...\n",
            "epoch 29 loss=1.3125\n",
            "Validation loss decreased (1.315174 --> 1.312452).  Saving model ...\n",
            "epoch 30 loss=1.3099\n",
            "Validation loss decreased (1.312452 --> 1.309857).  Saving model ...\n",
            "epoch 31 loss=1.3074\n",
            "Validation loss decreased (1.309857 --> 1.307430).  Saving model ...\n",
            "epoch 32 loss=1.3051\n",
            "Validation loss decreased (1.307430 --> 1.305086).  Saving model ...\n",
            "epoch 33 loss=1.3028\n",
            "Validation loss decreased (1.305086 --> 1.302799).  Saving model ...\n",
            "epoch 34 loss=1.3008\n",
            "Validation loss decreased (1.302799 --> 1.300806).  Saving model ...\n",
            "epoch 35 loss=1.2987\n",
            "Validation loss decreased (1.300806 --> 1.298687).  Saving model ...\n",
            "epoch 36 loss=1.2967\n",
            "Validation loss decreased (1.298687 --> 1.296671).  Saving model ...\n",
            "epoch 37 loss=1.2949\n",
            "Validation loss decreased (1.296671 --> 1.294860).  Saving model ...\n",
            "epoch 38 loss=1.2929\n",
            "Validation loss decreased (1.294860 --> 1.292925).  Saving model ...\n",
            "epoch 39 loss=1.2912\n",
            "Validation loss decreased (1.292925 --> 1.291167).  Saving model ...\n",
            "epoch 40 loss=1.2894\n",
            "Validation loss decreased (1.291167 --> 1.289433).  Saving model ...\n",
            "epoch 41 loss=1.2878\n",
            "Validation loss decreased (1.289433 --> 1.287787).  Saving model ...\n",
            "epoch 42 loss=1.2861\n",
            "Validation loss decreased (1.287787 --> 1.286102).  Saving model ...\n",
            "epoch 43 loss=1.2845\n",
            "Validation loss decreased (1.286102 --> 1.284460).  Saving model ...\n",
            "epoch 44 loss=1.2829\n",
            "Validation loss decreased (1.284460 --> 1.282904).  Saving model ...\n",
            "epoch 45 loss=1.2813\n",
            "Validation loss decreased (1.282904 --> 1.281294).  Saving model ...\n",
            "epoch 46 loss=1.2798\n",
            "Validation loss decreased (1.281294 --> 1.279768).  Saving model ...\n",
            "epoch 47 loss=1.2782\n",
            "Validation loss decreased (1.279768 --> 1.278245).  Saving model ...\n",
            "epoch 48 loss=1.2768\n",
            "Validation loss decreased (1.278245 --> 1.276755).  Saving model ...\n",
            "epoch 49 loss=1.2753\n",
            "Validation loss decreased (1.276755 --> 1.275312).  Saving model ...\n",
            "epoch 50 loss=1.2739\n",
            "Validation loss decreased (1.275312 --> 1.273865).  Saving model ...\n",
            "epoch 51 loss=1.2724\n",
            "Validation loss decreased (1.273865 --> 1.272426).  Saving model ...\n",
            "epoch 52 loss=1.2710\n",
            "Validation loss decreased (1.272426 --> 1.271010).  Saving model ...\n",
            "epoch 53 loss=1.2697\n",
            "Validation loss decreased (1.271010 --> 1.269670).  Saving model ...\n",
            "epoch 54 loss=1.2682\n",
            "Validation loss decreased (1.269670 --> 1.268154).  Saving model ...\n",
            "epoch 55 loss=1.2669\n",
            "Validation loss decreased (1.268154 --> 1.266883).  Saving model ...\n",
            "epoch 56 loss=1.2656\n",
            "Validation loss decreased (1.266883 --> 1.265575).  Saving model ...\n",
            "epoch 57 loss=1.2642\n",
            "Validation loss decreased (1.265575 --> 1.264214).  Saving model ...\n",
            "epoch 58 loss=1.2628\n",
            "Validation loss decreased (1.264214 --> 1.262836).  Saving model ...\n",
            "epoch 59 loss=1.2615\n",
            "Validation loss decreased (1.262836 --> 1.261475).  Saving model ...\n",
            "epoch 60 loss=1.2601\n",
            "Validation loss decreased (1.261475 --> 1.260146).  Saving model ...\n",
            "epoch 61 loss=1.2589\n",
            "Validation loss decreased (1.260146 --> 1.258892).  Saving model ...\n",
            "epoch 62 loss=1.2576\n",
            "Validation loss decreased (1.258892 --> 1.257568).  Saving model ...\n",
            "epoch 63 loss=1.2562\n",
            "Validation loss decreased (1.257568 --> 1.256199).  Saving model ...\n",
            "epoch 64 loss=1.2549\n",
            "Validation loss decreased (1.256199 --> 1.254890).  Saving model ...\n",
            "epoch 65 loss=1.2536\n",
            "Validation loss decreased (1.254890 --> 1.253616).  Saving model ...\n",
            "epoch 66 loss=1.2523\n",
            "Validation loss decreased (1.253616 --> 1.252337).  Saving model ...\n",
            "epoch 67 loss=1.2510\n",
            "Validation loss decreased (1.252337 --> 1.251012).  Saving model ...\n",
            "epoch 68 loss=1.2498\n",
            "Validation loss decreased (1.251012 --> 1.249766).  Saving model ...\n",
            "epoch 69 loss=1.2485\n",
            "Validation loss decreased (1.249766 --> 1.248492).  Saving model ...\n",
            "epoch 70 loss=1.2472\n",
            "Validation loss decreased (1.248492 --> 1.247245).  Saving model ...\n",
            "epoch 71 loss=1.2459\n",
            "Validation loss decreased (1.247245 --> 1.245905).  Saving model ...\n",
            "epoch 72 loss=1.2446\n",
            "Validation loss decreased (1.245905 --> 1.244598).  Saving model ...\n",
            "epoch 73 loss=1.2433\n",
            "Validation loss decreased (1.244598 --> 1.243345).  Saving model ...\n",
            "epoch 74 loss=1.2421\n",
            "Validation loss decreased (1.243345 --> 1.242115).  Saving model ...\n",
            "epoch 75 loss=1.2408\n",
            "Validation loss decreased (1.242115 --> 1.240769).  Saving model ...\n",
            "epoch 76 loss=1.2395\n",
            "Validation loss decreased (1.240769 --> 1.239472).  Saving model ...\n",
            "epoch 77 loss=1.2382\n",
            "Validation loss decreased (1.239472 --> 1.238234).  Saving model ...\n",
            "epoch 78 loss=1.2369\n",
            "Validation loss decreased (1.238234 --> 1.236926).  Saving model ...\n",
            "epoch 79 loss=1.2356\n",
            "Validation loss decreased (1.236926 --> 1.235617).  Saving model ...\n",
            "epoch 80 loss=1.2344\n",
            "Validation loss decreased (1.235617 --> 1.234381).  Saving model ...\n",
            "epoch 81 loss=1.2331\n",
            "Validation loss decreased (1.234381 --> 1.233123).  Saving model ...\n",
            "epoch 82 loss=1.2318\n",
            "Validation loss decreased (1.233123 --> 1.231797).  Saving model ...\n",
            "epoch 83 loss=1.2305\n",
            "Validation loss decreased (1.231797 --> 1.230547).  Saving model ...\n",
            "epoch 84 loss=1.2292\n",
            "Validation loss decreased (1.230547 --> 1.229195).  Saving model ...\n",
            "epoch 85 loss=1.2279\n",
            "Validation loss decreased (1.229195 --> 1.227919).  Saving model ...\n",
            "epoch 86 loss=1.2266\n",
            "Validation loss decreased (1.227919 --> 1.226624).  Saving model ...\n",
            "epoch 87 loss=1.2255\n",
            "Validation loss decreased (1.226624 --> 1.225490).  Saving model ...\n",
            "epoch 88 loss=1.2242\n",
            "Validation loss decreased (1.225490 --> 1.224168).  Saving model ...\n",
            "epoch 89 loss=1.2227\n",
            "Validation loss decreased (1.224168 --> 1.222701).  Saving model ...\n",
            "epoch 90 loss=1.2215\n",
            "Validation loss decreased (1.222701 --> 1.221534).  Saving model ...\n",
            "epoch 91 loss=1.2202\n",
            "Validation loss decreased (1.221534 --> 1.220238).  Saving model ...\n",
            "epoch 92 loss=1.2189\n",
            "Validation loss decreased (1.220238 --> 1.218904).  Saving model ...\n",
            "epoch 93 loss=1.2175\n",
            "Validation loss decreased (1.218904 --> 1.217540).  Saving model ...\n",
            "epoch 94 loss=1.2163\n",
            "Validation loss decreased (1.217540 --> 1.216290).  Saving model ...\n",
            "epoch 95 loss=1.2150\n",
            "Validation loss decreased (1.216290 --> 1.215007).  Saving model ...\n",
            "epoch 96 loss=1.2137\n",
            "Validation loss decreased (1.215007 --> 1.213653).  Saving model ...\n",
            "epoch 97 loss=1.2124\n",
            "Validation loss decreased (1.213653 --> 1.212399).  Saving model ...\n",
            "epoch 98 loss=1.2111\n",
            "Validation loss decreased (1.212399 --> 1.211127).  Saving model ...\n",
            "epoch 99 loss=1.2098\n",
            "Validation loss decreased (1.211127 --> 1.209818).  Saving model ...\n",
            "model saved to ae_Adam_multiview.pkl.\n",
            "load pretrained ae from \n",
            ":acc=0.9783 , nmi=0.9284 , ari=0.9465\n",
            "Iter 0 :Acc 0.9783 , nmi 0.9284 , ari 0.9465\n",
            "Iter 1 :Acc 0.9806 , nmi 0.9356 , ari 0.9524\n",
            "Iter 2 :Acc 0.9787 , nmi 0.9317 , ari 0.9480\n",
            "Iter 3 :Acc 0.9877 , nmi 0.9580 , ari 0.9696\n",
            "Iter 4 :Acc 0.9871 , nmi 0.9579 , ari 0.9680\n",
            "Iter 5 :Acc 0.9869 , nmi 0.9589 , ari 0.9674\n",
            "Iter 6 :Acc 0.9900 , nmi 0.9682 , ari 0.9752\n",
            "Iter 7 :Acc 0.9849 , nmi 0.9514 , ari 0.9630\n",
            "Iter 8 :Acc 0.9861 , nmi 0.9588 , ari 0.9655\n",
            "Iter 9 :Acc 0.9877 , nmi 0.9611 , ari 0.9694\n",
            "ACC_MAX:0.9900195694716243\n",
            "NMI_MAX:0.968159271826845\n",
            "ARI_MAX:0.9752128729336743\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "can't convert cuda:1 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m                                     dataset \u001b[38;5;241m=\u001b[39m MnistDataset()\n\u001b[1;32m     44\u001b[0m                                 \u001b[38;5;28mprint\u001b[39m(args)\n\u001b[0;32m---> 45\u001b[0m                                 \u001b[43mtrain_idec\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#                                     for name in net.state_dict():\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m#                                         print(name)\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[7], line 133\u001b[0m, in \u001b[0;36mtrain_idec\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mARI_MAX:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mARI[index_max]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m#     print(f\"z_finetuning:{Z_ALL[index_max]}\",f\"z_finetuning.shape:{Z_ALL[index_max].shape}\")\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m#     z_finetuning = Z_ALL[index_max].cpu().data.numpy()\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m#     np.savetxt('_seed2023_finetuning_multiview_可视化F44.csv', z_finetuning, delimiter=',')\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m#     print(f\"C_weights:{C_WEIGHTS[(5110*index_max):(5110*(index_max+1))]}\")\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m#     C_weights = np.array(C_WEIGHTS[(5110*(index_max+1)-10):(5110*(index_max+1))])#.reshape(5110,8)\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     C_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mC_WEIGHTS\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_max\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_max\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.reshape(5110,8)\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(C_weights[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    135\u001b[0m     file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfffffggggg_seed2023_IDEC_multiview_通道权重F1.csv\u001b[39m\u001b[38;5;124m'\u001b[39m    \u001b[38;5;66;03m#保存文件位置，即当前工作路径下的csv文件\u001b[39;00m\n",
            "File \u001b[0;32m~/.conda/envs/hfut3304/lib/python3.8/site-packages/torch/_tensor.py:678\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:1 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # 超参数设置    # F1:100 0.001 0.001    F2:100 0.001 0.001    F3:1000 0.01 0.1    F4:1000 0.001 0.1\n",
        "    setup_seed(2023)\n",
        "    for gamma in [100]: # 1000,100,10,1,0.1,0.01,0.001\n",
        "        for lr in [0.0001]:\n",
        "            for h_z in [20]: # 10, 20, 40, 80, 160\n",
        "                for layer1 in [500]: # 500, 800, 1200, 1600, 2000, 2400\n",
        "                    for layer2 in [1600]: # 1000, 1600, 2400, 3200, 4000, 4800\n",
        "                        for lambda1 in [0.001]: # 0.001,0.01,0.1,1,10,100\n",
        "                            for lambda2 in [0.001]: # 0.001,0.01,0.1,1,10,100\n",
        "                                for i in range(10):\n",
        "\n",
        "                                    parser = argparse.ArgumentParser(\n",
        "                                        description='train',\n",
        "                                        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "                                    parser.add_argument('--lambda1', type=float, default=lambda1)\n",
        "                                    parser.add_argument('--lambda2', type=float, default=lambda2)\n",
        "                                    parser.add_argument('--layer1', type=int, default=layer1)\n",
        "                                    parser.add_argument('--layer2', type=int, default=layer2)\n",
        "                                    parser.add_argument('--lr', type=float, default=lr)\n",
        "                                    parser.add_argument('--in_channels', type=int, default=8)\n",
        "                                    parser.add_argument('--n_clusters', default=5, type=int)\n",
        "                                    parser.add_argument('--batch_size', default=256, type=int)\n",
        "                                    parser.add_argument('--n_z', default=h_z, type=int)\n",
        "                                    parser.add_argument('--dataset', type=str, default='mnist')\n",
        "                                    parser.add_argument('--pretrain_path', type=str, default='ae_Adam_multiview')\n",
        "                                    parser.add_argument(\n",
        "                                        '--gamma',\n",
        "                                        default=gamma,\n",
        "                                        type=float,\n",
        "                                        help='coefficient of clustering loss')\n",
        "                                    parser.add_argument('--update_interval', default=1, type=int)\n",
        "                                    parser.add_argument('--tol', default=0.001, type=float)\n",
        "                                    args = parser.parse_known_args()[0]\n",
        "                                    args.cuda = torch.cuda.is_available()\n",
        "                                    print(\"use cuda: {}\".format(args.cuda))\n",
        "    #                                     device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "                                    device = torch.device('cuda:1')\n",
        "                                    if args.dataset == 'mnist':\n",
        "                                        args.pretrain_path = 'ae_Adam_multiview.pkl'\n",
        "                                        args.n_clusters = 5\n",
        "                                        args.n_input = 2048\n",
        "                                        dataset = MnistDataset()\n",
        "                                    print(args)\n",
        "                                    train_idec()\n",
        "    #                                     for name in net.state_dict():\n",
        "    #                                         print(name)\n",
        "\n"
      ],
      "id": "CwDnqvBwsPXT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABDkyzDKsPXU"
      },
      "outputs": [],
      "source": [],
      "id": "ABDkyzDKsPXU"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}